###############################################################################
#
#  Copyright (c) 2021-2024 Intel Corporation
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
###############################################################################


_H_HEADER = """// Autogenerated file by {gen}. Do not edit directly!

#pragma once
#include <ATen/Tensor.h>
#include <ATen/core/function_schema.h>
#include "habana_kernels/lazy_kernels.h"
#include "hpu_ops/hpu_op_helper.h"

namespace habana {{
{op_backend_classes}
{op_frontend_classes}
{header_decls}
}}  // namespace habana
"""


_H_HEADER_EAGER = """// Autogenerated file by {gen}. Do not edit directly!

#pragma once
#include <ATen/Tensor.h>

#include "habana_eager/ops/eager_op.h"
#include "hpu_ops/hpu_op_helper.h"

namespace habana {{
{op_backend_classes}
{op_frontend_classes}
{header_decls}
}}  // namespace habana
"""

_CPP_HEADER = """// Autogenerated file by {gen}. Do not edit directly!
{header_inclusions}

using habana_helpers::DTypeHelper;
using synapse_helpers::graph;
using torch::jit::Stack;


namespace habana {{

{dtype_defs}

{funcs}

{op_backend}

static const auto& kr_gen_{file_idx} = KernelRegistry()
{kr_regs};

{torch_regs}

{custom_schema_regs}
}}  // namespace habana
"""


_CPP_HEADER_CHECK_KERNEL_SUPPORT = """// Autogenerated file by {gen}. Do not edit directly!
{header_inclusions}

namespace habana {{

{dtype_defs}

{funcs}

{op_backend}

}}  // namespace habana
"""


_OPCLASS_HEADER = """struct {cname} : {op_backend_class} {{
  {cname}(int device_id, c10::ScalarType scalar_type) :
      {op_backend_class}(device_id, \"{guid}\", scalar_type, {{{out_ids}}}, {{{inplace_ids}}}, {{{scalar_ids}}}, {is_out_fn}) {{{ctor_extra_calls}
  }}{custom_handler}
}};
"""

_CUSTOM_HANDLER = """

  void CustomHandler(graph &g, Stack& stack) override {{
    static_cast<void>(g);
    static_cast<void>(stack);
    {body}
  }}"""

_FILL_PARAMS = """[](const at::Stack& stack, size_t& size) {{
            using T = {ns_param};
            size = sizeof(T);
            return std::make_shared<T>(T{{{args}}});
        }}"""

_AUTOCAST_TEMPLATE = """// Autogenerated file by {gen}. Do not edit directly!
/**
* Copyright (c) 2021-2024 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

#include "pytorch_helpers/habana_helpers/pt_version_check.h"

#include <ATen/ATen.h>
#include <ATen/NativeFunctions.h>
#include <torch/library.h>
#include "hpu_ops/autocast_helpers.h"

namespace at {{
namespace autocast {{
namespace {{

using tuple_2_tensors = std::tuple<Tensor, Tensor>;
using tuple_3_tensors = std::tuple<Tensor, Tensor, Tensor>;
using tuple_4_tensors = std::tuple<Tensor, Tensor, Tensor, Tensor>;
using tuple_5_tensors = std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor>;
using tuple_6_tensors = std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor>;
using tuple_4_tensors_int64 = std::tuple<Tensor,Tensor,Tensor,Tensor,int64_t>;
using tuple_2_tensors_double_int64 = std::tuple<Tensor,Tensor,double,int64_t>;
using tuple_3_tensors_vector = std::tuple<Tensor,Tensor,Tensor,::std::vector<Tensor>>;
using tuple_double_int64 = std::tuple<double,int64_t>;
using tuple_tensor_vector = std::tuple<Tensor,::std::vector<Tensor>>;
using tuple_vector_tensor = std::tuple<::std::vector<Tensor>,Tensor>;
using tuple_tensor_2_vectors = std::tuple<Tensor,::std::vector<Tensor>,::std::vector<Tensor>>;
using tuple_4_tensors_2_int64_2_tensors = std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t,int64_t,at::Tensor,at::Tensor>;
using tuple_4_tensors_4_int64_tensor = std::tuple<Tensor,Tensor,Tensor,Tensor,int64_t,int64_t,int64_t,int64_t,Tensor>;
using tuple_2_tensors_2_int64_tensor = std::tuple<Tensor,Tensor,int64_t,int64_t,Tensor>;
using tuple_4_tensors_2_int64_3_tensor = std::tuple<Tensor,Tensor,Tensor,Tensor,int64_t,int64_t,Tensor,Tensor,Tensor>;
using tuple_4_tensors_2_int64 = std::tuple<Tensor,Tensor,Tensor,Tensor,int64_t,int64_t>;
using tuple_3_vectors = std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>>;
using tuple_5_vectors = std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>>;
using tuple_4_vectors = std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>>;
{fallback_fallthrough}
TORCH_LIBRARY_IMPL(aten, AutocastHPU, m) {{
{ops_registrations}
}}

}} // namespace
}} // namespace autocast
}} // namespace at
"""

_AUTOCAST_FALLBACK = """
TORCH_LIBRARY_IMPL(_, AutocastHPU, m) {{
  m.fallback(torch::CppFunction::makeFallthrough());
}}
"""

_HPU_WRAP_REGS = """// Autogenerated file by {gen}. Do not edit directly!
#include "wrap_kernels_declarations.h"

namespace habana {{
{regs}
}}  // namespace habana
"""

_HPU_WRAP_HEADERS = """// Autogenerated file by {gen}. Do not edit directly!

#pragma once
#include <ATen/ExpandUtils.h>
#include <torch/script.h>
#include "pytorch_helpers/habana_helpers/pt_version_check.h"

namespace hpu_wrap {{

{headers}

}} // namespace hpu_wrap
"""

_STACK_POP_CODE_FORMAT_ = """
    {}
      torch::jit::pop(stack{});
      auto is_supported = impl({}, is_dynamic);
      return is_supported;
    }}
"""

_TORCHGEN_POP_CODE_FORMAT_ = """
    {}
      auto is_supported = impl({}, is_dynamic);
      return is_supported;
    }}
"""
