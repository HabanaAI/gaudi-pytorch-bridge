// Autogenerated file by gen_op.py. Do not edit directly!

#include "hpu_ops/cpu_fallback.h"
#include "hpu_ops/op_validator.h"
#include "hpu_ops/op_logger.h"
#include "common/dump_args.h"
#include "habana_kernels/lazy_kernels_declarations.h"
#include "hpu_ops/lazy/reduction_template.h"
#include "habana_lazy/hpu_stage_submission.h"
using habana_lazy::LazyOp;
using habana_lazy::GraphHashBuilder;

#include "softmax_fp8.h"


using habana_helpers::DTypeHelper;
using synapse_helpers::graph;
using torch::jit::Stack;


namespace habana {



at::Tensor softmax_fp8(const at::Tensor & input, int64_t dim, const c10::optional<at::Tensor> & input_scale, const c10::optional<at::Tensor> & output_scale, const c10::optional<at::Tensor> & inv_attn_heads, const c10::optional<at::Tensor> & fused_add) {
  PT_LAZY_OP_TRACE;
  PT_LAZY_TRACE;
  PT_OP_INFO("softmax_fp8: ", DUMP_6ARGS(input, dim, input_scale, output_scale, inv_attn_heads, fused_add));

  [[maybe_unused]] bool require_h2d = false;
  [[maybe_unused]] bool require_st = false;

  LazyOp<at::Tensor> hpu_op{"hpu::softmax_fp8", {input, dim, input_scale, output_scale, inv_attn_heads, fused_add}};
  hpu_op.SetOutputMetaFn(SoftmaxFp8Meta);
  RUN_MAYBE_WITH_ACC_THREAD(softmax_fp8, hpu_op);
}





static const auto& kr_gen__custom = KernelRegistry()
;

TORCH_LIBRARY_IMPL(hpu, HPU, m) {
  m.impl("softmax_fp8", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &)>(&habana::softmax_fp8));

}



}  // namespace habana

