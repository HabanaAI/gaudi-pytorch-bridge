// Autogenerated file by gen_op.py. Do not edit directly!

#include "hpu_ops/op_validator.h"
#include "hpu_ops/backend/reduction_template.h"
#include "softmax_fp8.h"


using habana_helpers::DTypeHelper;
using synapse_helpers::graph;
using torch::jit::Stack;


namespace habana {





struct Gensoftmax_fp8 : SoftmaxFp8 {
  Gensoftmax_fp8(int device_id, c10::ScalarType scalar_type) :
      SoftmaxFp8(device_id, "softmax_fwd", scalar_type, {0}, {}, {}, false) {
        SetOutputMetaFn(SoftmaxFp8Meta);
  }
};



static const auto& kr_gen__custom = KernelRegistry()
.REGISTER_HPU_BACKEND("hpu::softmax_fp8", Gensoftmax_fp8)
;



TORCH_LIBRARY_FRAGMENT(hpu, m) {
  static_cast<void>(m);
  m.def("hpu::softmax_fp8(Tensor input, int dim, Tensor? input_scale=None, Tensor? output_scale=None, Tensor? inv_attn_heads=None, Tensor? fused_add=None) -> Tensor");

}
}  // namespace habana

