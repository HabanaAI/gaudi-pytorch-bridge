// Autogenerated file by gen_op.py. Do not edit directly!

#include <torch/extension.h>
#include <pybind11/stl.h>
#include <pybind11/pybind11.h>
#include <torch/csrc/jit/tensorexpr/tensorexpr_init.h>
#include <torch/csrc/jit/python/pybind_utils.h>
#include "cpu_fallback.h"

using habana_helpers::DTypeHelper;
using namespace torch::jit;


namespace habana {



struct shared_layer_bitwise_left_shift : SharedLayerOp {
bool func(torch::jit::Stack &stack, bool is_dynamic) {
  if (stack.size() == 2) {
    auto ivalue_arr = torch::jit::last(stack, 2);
    if (ivalue_arr[0].isTensor() && ivalue_arr[1].isScalar() ) {

      c10::IValue self = std::move(peek(stack, 0, 2));
      c10::IValue other = std::move(peek(stack, 1, 2));

      at::Tensor self_base = self.to<at::Tensor>();
      at::Scalar other_base = other.to<at::Scalar>();
      auto is_supported = impl(self_base, other_base, is_dynamic);
      return is_supported;
    }
  }
  return false;
}
private:
bool impl(const at::Tensor & self, const at::Scalar & other, bool is_dynamic) {
  HPU_SUPPORTED_DTYPES(({{-1, {at::kInt, at::kChar, at::kByte, at::kShort, at::kBool}}}))
  RETURN_IF_UNSUPPORTED_DTYPE2(self, bitwise_left_shift, is_dynamic, Tensor_Scalar, self, other)

  return true;
}

};

struct shared_layer__native_batch_norm_legit : SharedLayerOp {
bool func(torch::jit::Stack &stack, bool is_dynamic) {
  if (stack.size() == 8) {
    auto ivalue_arr = torch::jit::last(stack, 8);
    if (ivalue_arr[0].isTensor() && ivalue_arr[3].isTensor() && ivalue_arr[4].isTensor() && ivalue_arr[5].isBool() && ivalue_arr[6].isDouble() && ivalue_arr[7].isDouble() ) {

      c10::IValue input = std::move(peek(stack, 0, 8));
      c10::IValue weight = std::move(peek(stack, 1, 8));
      c10::IValue bias = std::move(peek(stack, 2, 8));
      c10::IValue running_mean = std::move(peek(stack, 3, 8));
      c10::IValue running_var = std::move(peek(stack, 4, 8));
      c10::IValue training = std::move(peek(stack, 5, 8));
      c10::IValue momentum = std::move(peek(stack, 6, 8));
      c10::IValue eps = std::move(peek(stack, 7, 8));

      at::Tensor input_base = input.to<at::Tensor>();

      auto weight_opt = weight.toOptional<c10::IValue>();
      ::std::optional<at::Tensor> weight_opt_out;
      if (weight_opt.has_value()) {
          const c10::IValue weight_opt_in = weight_opt.value();
          at::Tensor weight_opt_in_base = weight_opt_in.to<at::Tensor>();
          weight_opt_out = ::std::optional<at::Tensor>(weight_opt_in_base);
      } else {
          weight_opt_out = ::std::optional<at::Tensor>();
      }


      auto bias_opt = bias.toOptional<c10::IValue>();
      ::std::optional<at::Tensor> bias_opt_out;
      if (bias_opt.has_value()) {
          const c10::IValue bias_opt_in = bias_opt.value();
          at::Tensor bias_opt_in_base = bias_opt_in.to<at::Tensor>();
          bias_opt_out = ::std::optional<at::Tensor>(bias_opt_in_base);
      } else {
          bias_opt_out = ::std::optional<at::Tensor>();
      }

      at::Tensor running_mean_base = running_mean.to<at::Tensor>();
      at::Tensor running_var_base = running_var.to<at::Tensor>();
      bool training_base = training.to<bool>();
      double momentum_base = momentum.to<double>();
      double eps_base = eps.to<double>();
      auto is_supported = impl(input_base, weight_opt_out, bias_opt_out, running_mean_base, running_var_base, training_base, momentum_base, eps_base, is_dynamic);
      return is_supported;
    }
  }
  return false;
}
private:
bool impl(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, at::Tensor & running_mean, at::Tensor & running_var, bool training, double momentum, double eps, bool is_dynamic) {
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kBFloat16, at::kFloat, at::kDouble}},
   {synDeviceGaudi2, {at::kBFloat16, at::kFloat, at::kHalf, at::kDouble}},
   {synDeviceGaudi3, {at::kBFloat16, at::kFloat, at::kHalf, at::kDouble}}}), input)
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kFloat, at::kDouble}},
   {synDeviceGaudi2, {at::kFloat, at::kDouble}},
   {synDeviceGaudi3, {at::kFloat, at::kDouble}}}), weight)
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kFloat, at::kDouble}},
   {synDeviceGaudi2, {at::kFloat, at::kDouble}},
   {synDeviceGaudi3, {at::kFloat, at::kDouble}}}), bias)
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kFloat, at::kDouble}},
   {synDeviceGaudi2, {at::kFloat, at::kDouble}},
   {synDeviceGaudi3, {at::kFloat, at::kDouble}}}), running_mean)
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kFloat, at::kDouble}},
   {synDeviceGaudi2, {at::kFloat, at::kDouble}},
   {synDeviceGaudi3, {at::kFloat, at::kDouble}}}), running_var)
  RETURN_IF_UNSUPPORTED_DTYPE_PER_TENSOR(input, _native_batch_norm_legit, is_dynamic, input, weight, bias, running_mean, running_var, training, momentum, eps)
  RETURN_IF_UNSUPPORTED_DTYPE_PER_TENSOR(running_mean, _native_batch_norm_legit, is_dynamic, input, weight, bias, running_mean, running_var, training, momentum, eps)
  RETURN_IF_UNSUPPORTED_DTYPE_PER_TENSOR(running_var, _native_batch_norm_legit, is_dynamic, input, weight, bias, running_mean, running_var, training, momentum, eps)

  return true;
}

};





}  // namespace habana

