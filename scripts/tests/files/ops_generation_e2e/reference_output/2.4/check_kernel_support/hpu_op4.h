// Autogenerated file by gen_op.py. Do not edit directly!

#include <torch/extension.h>
#include <pybind11/stl.h>
#include <pybind11/pybind11.h>
#include <torch/csrc/jit/tensorexpr/tensorexpr_init.h>
#include <torch/csrc/jit/python/pybind_utils.h>
#include "cpu_fallback.h"

using habana_helpers::DTypeHelper;
using namespace torch::jit;


namespace habana {



struct shared_layer_prod_out : SharedLayerOp {
bool func(torch::jit::Stack &stack, bool is_dynamic) {
  if (stack.size() == 5) {
    auto ivalue_arr = torch::jit::last(stack, 5);
    if (ivalue_arr[0].isTensor() && ivalue_arr[1].isInt() && ivalue_arr[2].isBool() && ivalue_arr[4].isTensor() ) {

      c10::IValue self = std::move(peek(stack, 0, 5));
      c10::IValue dim = std::move(peek(stack, 1, 5));
      c10::IValue keepdim = std::move(peek(stack, 2, 5));
      c10::IValue dtype = std::move(peek(stack, 3, 5));
      c10::IValue out = std::move(peek(stack, 4, 5));

      at::Tensor self_base = self.to<at::Tensor>();
      int64_t dim_base = dim.to<int64_t>();
      bool keepdim_base = keepdim.to<bool>();

      auto dtype_opt = dtype.toOptional<c10::IValue>();
      ::std::optional<at::ScalarType> dtype_opt_out;
      if (dtype_opt.has_value()) {
          const c10::IValue dtype_opt_in = dtype_opt.value();
          at::ScalarType dtype_opt_in_base = dtype_opt_in.to<at::ScalarType>();
          dtype_opt_out = ::std::optional<at::ScalarType>(dtype_opt_in_base);
      } else {
          dtype_opt_out = ::std::optional<at::ScalarType>();
      }

      at::Tensor out_base = out.to<at::Tensor>();
      auto is_supported = impl(self_base, dim_base, keepdim_base, dtype_opt_out, out_base, is_dynamic);
      return is_supported;
    }
  }
  return false;
}
private:
bool impl(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out, bool is_dynamic) {
  auto compute_type = DTypeHelper::get_compute_dtype({self}, out, DTypeHelper::DtypePromoteVariant::kReduction, false/*safe_cast*/, dtype);
  static_cast<void>(compute_type);

  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kBFloat16, at::kFloat, at::kChar, at::kByte, at::kShort, at::kInt, at::kDouble, at::kBool}},
   {synDeviceGaudi2, {at::kBFloat16, at::kFloat, at::kChar, at::kByte, at::kShort, at::kInt, at::kHalf, at::kDouble, at::kBool}},
   {synDeviceGaudi3, {at::kBFloat16, at::kFloat, at::kChar, at::kByte, at::kShort, at::kInt, at::kHalf, at::kDouble, at::kBool}}}))
  RETURN_IF_UNSUPPORTED_DTYPE2(compute_type, prod, is_dynamic, int_out, self, dim, keepdim, dtype, out)

  return true;
}

};

struct shared_layer_clone : SharedLayerOp {
bool func(torch::jit::Stack &stack, bool is_dynamic) {
  if (stack.size() == 2) {
    auto ivalue_arr = torch::jit::last(stack, 2);
    if (ivalue_arr[0].isTensor() ) {

      c10::IValue self = std::move(peek(stack, 0, 2));
      c10::IValue memory_format = std::move(peek(stack, 1, 2));

      at::Tensor self_base = self.to<at::Tensor>();

      auto memory_format_opt = memory_format.toOptional<c10::IValue>();
      ::std::optional<at::MemoryFormat> memory_format_opt_out;
      if (memory_format_opt.has_value()) {
          const c10::IValue memory_format_opt_in = memory_format_opt.value();
          at::MemoryFormat memory_format_opt_in_base = memory_format_opt_in.to<at::MemoryFormat>();
          memory_format_opt_out = ::std::optional<at::MemoryFormat>(memory_format_opt_in_base);
      } else {
          memory_format_opt_out = ::std::optional<at::MemoryFormat>();
      }

      auto is_supported = impl(self_base, memory_format_opt_out, is_dynamic);
      return is_supported;
    }
  }
  return false;
}
private:
bool impl(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, bool is_dynamic) {
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kBFloat16, at::kFloat, at::kInt, at::kChar, at::kByte, at::kShort, at::kDouble, at::kBool}},
   {synDeviceGaudi2, {at::kBFloat16, at::kFloat, at::kInt, at::kChar, at::kByte, at::kShort, at::kHalf, at::kFloat8_e5m2, at::kFloat8_e4m3fn, at::kDouble, at::kBool}},
   {synDeviceGaudi3, {at::kBFloat16, at::kFloat, at::kInt, at::kChar, at::kByte, at::kShort, at::kHalf, at::kFloat8_e5m2, at::kFloat8_e4m3fn, at::kDouble, at::kBool}}}))
  RETURN_IF_UNSUPPORTED_DTYPE(self, clone, is_dynamic, self, memory_format)

  return true;
}

};





}  // namespace habana

