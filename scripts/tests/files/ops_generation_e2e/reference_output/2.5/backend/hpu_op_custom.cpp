// Autogenerated file by gen_op.py. Do not edit directly!

#include "hpu_ops/op_validator.h"
#include "exp_fast_math.h"
#include "softmax_fp8.h"


using habana_helpers::DTypeHelper;
using synapse_helpers::graph;
using torch::jit::Stack;


namespace habana {





struct Gensoftmax_fp8 : SoftmaxFp8 {
  Gensoftmax_fp8(int device_id, c10::ScalarType scalar_type) :
      SoftmaxFp8(device_id, "softmax_fwd", scalar_type, {0}, {}, {}, false) {
        SetOutputMetaFn(SoftmaxFp8Meta);
  }
};

struct Genexp_fast_math : OpBackend {
  Genexp_fast_math(int device_id, c10::ScalarType scalar_type) :
      OpBackend(device_id, "exp_fast_math_fwd", scalar_type, {0}, {}, {}, false) {
  }
};



static const auto& kr_gen__custom = KernelRegistry()
.REGISTER_HPU_BACKEND("hpu::softmax_fp8", Gensoftmax_fp8)
.REGISTER_HPU_BACKEND("hpu::exp_fast_math", Genexp_fast_math)
;



TORCH_LIBRARY_FRAGMENT(hpu, m) {
  static_cast<void>(m);
  m.def("hpu::softmax_fp8(Tensor input, int dim, Tensor? input_scale=None, Tensor? output_scale=None, Tensor? inv_attn_heads=None, Tensor? fused_add=None) -> Tensor");
  m.def("hpu::exp_fast_math(Tensor self) -> Tensor");

}
}  // namespace habana

