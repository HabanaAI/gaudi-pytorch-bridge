// Autogenerated file by gen_op.py. Do not edit directly!

#include "hpu_ops/cpu_fallback.h"
#include "hpu_ops/op_validator.h"
#include "hpu_ops/op_logger.h"
#include "common/dump_args.h"
#include "habana_kernels/lazy_kernels_declarations.h"
#include "habana_kernels/lazy_kernels.h"
#include "habana_lazy/hpu_stage_submission.h"
using habana_lazy::LazyOp;
using habana_lazy::GraphHashBuilder;

#include "quantize_per_channel.h"


using habana_helpers::DTypeHelper;
using synapse_helpers::graph;
using torch::jit::Stack;


namespace habana {



at::Tensor quantize_per_channel(const at::Tensor & input, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, at::ScalarType type) {
  PT_LAZY_OP_TRACE;
  PT_LAZY_TRACE;
  PT_OP_INFO("quantize_per_channel: ", DUMP_7ARGS(input, scales, zero_points, axis, quant_min, quant_max, type));

  [[maybe_unused]] bool require_h2d = false;
  [[maybe_unused]] bool require_st = false;

  LazyOp<at::Tensor> hpu_op{"quantized_decomposed::quantize_per_channel", {input, scales, zero_points, axis, quant_min, quant_max, type}};
  hpu_op.SetOutputMetaFn(QuantizePerChannelMeta);
  RUN_MAYBE_WITH_ACC_THREAD(quantize_per_channel, hpu_op);
}





static const auto& kr_gen__quant = KernelRegistry()
;

TORCH_LIBRARY_IMPL(quantized_decomposed, HPU, m) {
  m.impl("quantize_per_channel", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, at::ScalarType)>(&habana::quantize_per_channel));

}



}  // namespace habana

