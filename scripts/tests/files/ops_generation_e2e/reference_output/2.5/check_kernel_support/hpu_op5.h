// Autogenerated file by gen_op.py. Do not edit directly!

#include <torch/extension.h>
#include <pybind11/stl.h>
#include <pybind11/pybind11.h>
#include <torch/csrc/jit/tensorexpr/tensorexpr_init.h>
#include <torch/csrc/jit/python/pybind_utils.h>
#include "cpu_fallback.h"

using habana_helpers::DTypeHelper;
using namespace torch::jit;


namespace habana {



struct shared_layer_mul_out : SharedLayerOp {
bool func(torch::jit::Stack &stack, bool is_dynamic) {
  if (stack.size() == 3) {
    auto ivalue_arr = torch::jit::last(stack, 3);
    if (ivalue_arr[0].isTensor() && ivalue_arr[1].isScalar() && ivalue_arr[2].isTensor() ) {

      c10::IValue self = std::move(peek(stack, 0, 3));
      c10::IValue other = std::move(peek(stack, 1, 3));
      c10::IValue out = std::move(peek(stack, 2, 3));

      at::Tensor self_base = self.to<at::Tensor>();
      at::Scalar other_base = other.to<at::Scalar>();
      at::Tensor out_base = out.to<at::Tensor>();
      auto is_supported = impl(self_base, other_base, out_base, is_dynamic);
      return is_supported;
    }
  }
  return false;
}
private:
bool impl(const at::Tensor & self, const at::Scalar & other, at::Tensor & out, bool is_dynamic) {
  auto compute_type = DTypeHelper::get_compute_dtype({self, other}, out, DTypeHelper::DtypePromoteVariant::kPromoteToCommon, true/*safe_cast*/);
  static_cast<void>(compute_type);

  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kBFloat16, at::kByte, at::kChar, at::kFloat, at::kInt, at::kShort, at::kDouble, at::kBool}},
   {synDeviceGaudi2, {at::kBFloat16, at::kByte, at::kChar, at::kFloat, at::kInt, at::kLong, at::kShort, at::kHalf, at::kFloat8_e5m2, at::kFloat8_e4m3fn, at::kDouble, at::kBool}},
   {synDeviceGaudi3, {at::kBFloat16, at::kByte, at::kChar, at::kFloat, at::kInt, at::kLong, at::kShort, at::kHalf, at::kFloat8_e5m2, at::kFloat8_e4m3fn, at::kDouble, at::kBool}}}))
  RETURN_IF_UNSUPPORTED_DTYPE2(compute_type, mul, is_dynamic, Scalar_out, self, other, out)

  return true;
}

};

struct shared_layer_sort_out : SharedLayerOp {
bool func(torch::jit::Stack &stack, bool is_dynamic) {
  if (stack.size() == 6) {
    auto ivalue_arr = torch::jit::last(stack, 6);
    if (ivalue_arr[0].isTensor() && ivalue_arr[2].isInt() && ivalue_arr[3].isBool() && ivalue_arr[4].isTensor() && ivalue_arr[5].isTensor() ) {

      c10::IValue self = std::move(peek(stack, 0, 6));
      c10::IValue stable = std::move(peek(stack, 1, 6));
      c10::IValue dim = std::move(peek(stack, 2, 6));
      c10::IValue descending = std::move(peek(stack, 3, 6));
      c10::IValue values = std::move(peek(stack, 4, 6));
      c10::IValue indices = std::move(peek(stack, 5, 6));

      at::Tensor self_base = self.to<at::Tensor>();

      auto stable_opt = stable.toOptional<c10::IValue>();
      ::std::optional<bool> stable_opt_out;
      if (stable_opt.has_value()) {
          const c10::IValue stable_opt_in = stable_opt.value();
          bool stable_opt_in_base = stable_opt_in.to<bool>();
          stable_opt_out = ::std::optional<bool>(stable_opt_in_base);
      } else {
          stable_opt_out = ::std::optional<bool>();
      }

      int64_t dim_base = dim.to<int64_t>();
      bool descending_base = descending.to<bool>();
      at::Tensor values_base = values.to<at::Tensor>();
      at::Tensor indices_base = indices.to<at::Tensor>();
      auto is_supported = impl(self_base, stable_opt_out, dim_base, descending_base, values_base, indices_base, is_dynamic);
      return is_supported;
    }
  }
  return false;
}
private:
bool impl(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices, bool is_dynamic) {
  HPU_SUPPORTED_DTYPES(({{synDeviceGaudi, {at::kFloat, at::kInt, at::kBFloat16, at::kShort, at::kDouble}},
   {synDeviceGaudi2, {at::kFloat, at::kInt, at::kLong, at::kBFloat16, at::kShort, at::kHalf, at::kDouble}},
   {synDeviceGaudi3, {at::kFloat, at::kInt, at::kLong, at::kBFloat16, at::kShort, at::kHalf, at::kDouble}}}))
  RETURN_IF_UNSUPPORTED_DTYPE2(self, sort, is_dynamic, values_stable, self, stable, dim, descending, values, indices)
  RETURN_IF_UNSUPPORTED_DTYPE2(values, sort, is_dynamic, values_stable, self, stable, dim, descending, values, indices)

  return true;
}

};





}  // namespace habana

