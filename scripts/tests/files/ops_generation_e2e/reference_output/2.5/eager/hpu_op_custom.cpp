// Autogenerated file by gen_op.py. Do not edit directly!
#include "hpu_ops/cpu_fallback.h"
#include "hpu_ops/op_validator.h"
#include "hpu_ops/op_logger.h"
#include "common/dump_args.h"
#include "habana_eager/eager_exec.h"
#include "habana_eager/ops/eager_op.h"
#include "habana_eager/ops/override_fns.h"
#include "exp_fast_math.h"
#include "softmax_fp8.h"


using habana_helpers::DTypeHelper;
using synapse_helpers::graph;
using torch::jit::Stack;


namespace habana {

static CheckNodeWithSharedLayerValidator validator_exp_fast_math("exp_fast_math", "exp_fast_math_fwd", {0}, {}, nullptr, {}, false, false, false, false);


at::Tensor softmax_fp8(const at::Tensor & input, int64_t dim, const c10::optional<at::Tensor> & input_scale, const c10::optional<at::Tensor> & output_scale, const c10::optional<at::Tensor> & inv_attn_heads, const c10::optional<at::Tensor> & fused_add) {
  PT_EAGER_TRACE;
  PT_OP_INFO("softmax_fp8: ", DUMP_6ARGS(input, dim, input_scale, output_scale, inv_attn_heads, fused_add));

  [[maybe_unused]] bool require_h2d = false;
  [[maybe_unused]] bool require_st = false;

  eager::EagerOp<at::Tensor> hpu_op{"hpu::softmax_fp8", {input, dim, input_scale, output_scale, inv_attn_heads, fused_add}};
  hpu_op.SetOutputMetaFn(SoftmaxFp8Meta);
  hpu_op.set_eager_op_info({eager::eagerOpKind::OutOfPlace, "hpu::softmax_fp8", require_h2d, require_st, decltype(eager::EagerOpMetaData::out_indices_){}});
  return hpu_op.call();
}

at::Tensor exp_fast_math(const at::Tensor & self) {
  PT_EAGER_TRACE;
  PT_OP_INFO("exp_fast_math: ", DUMP_ARG(self));

  [[maybe_unused]] bool require_h2d = false;
  [[maybe_unused]] bool require_st = false;

  VAL_FAIL_CUSTOM_IF_UNSUPPORTED_DTYPE(exp_fast_math, true, self)

  eager::EagerOp<at::Tensor> hpu_op{"hpu::exp_fast_math", {self}};
  hpu_op.set_eager_op_info({eager::eagerOpKind::OutOfPlace, "hpu::exp_fast_math", require_h2d, require_st, decltype(eager::EagerOpMetaData::out_indices_){}});
  return hpu_op.call();
}





static const auto& kr_gen__custom = KernelRegistry()
;

TORCH_LIBRARY_IMPL(hpu, HPU, m) {
  m.impl("softmax_fp8", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &)>(&habana::softmax_fp8));
  m.impl("exp_fast_math", static_cast<at::Tensor (*)(const at::Tensor &)>(&habana::exp_fast_math));

}



}  // namespace habana

